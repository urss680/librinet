<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Librillaia Pensamiento profundo</title>
    <style>
        body, html {
            height: 100%;
            margin: 0;
            overflow: hidden;
        }

        body {
            font-family: 'Inter', sans-serif;
            background: linear-gradient(-45deg, #ee7752, #e73c7e, #23a6d5, #23d5ab, #d5b823, #d53223);
            background-size: 400% 400%;
            animation: gradient-animation 15s ease infinite;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            color: #f8fafc;
        }
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');

        @keyframes gradient-animation {
            0% { background-position: 0% 50%; }
            50% { background-position: 100% 50%; }
            100% { background-position: 0% 50%; }
        }

        .voice-interface {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 2rem;
            width: 90%;
            max-width: 600px;
            padding: 2rem;
            background: rgba(0, 0, 0, 0.4);
            border-radius: 1.5rem;
            backdrop-filter: blur(10px);
            box-shadow: 0 4px 30px rgba(0, 0, 0, 0.2);
            border: 1px solid rgba(255, 255, 255, 0.3);
        }
        
        .infinity-symbol {
            width: 80px;
            height: 40px;
            display: flex;
            justify-content: center;
            align-items: center;
            font-size: 4rem;
            font-weight: 200;
            color: #f8fafc;
        }

        #transcription-display {
            background: rgba(255, 255, 255, 0.1);
            color: #f8fafc;
            border: 1px solid rgba(255, 255, 255, 0.3);
            border-radius: 0.75rem;
            height: 3rem;
            padding: 1rem;
            font-size: 1rem;
            line-height: 1.5rem;
            outline: none;
            transition: border-color 0.2s ease;
            width: 100%;
            box-sizing: border-box;
            text-align: center;
        }
        #transcription-display::placeholder {
            color: #d1d5db;
        }
        
        .mic-button {
            background-color: transparent;
            color: #3b82f6;
            padding: 1.5rem;
            border-radius: 50%;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            border: 2px solid #3b82f6;
            transition: background-color 0.2s ease, color 0.2s ease, transform 0.2s ease;
        }
        .mic-button:hover {
            background-color: #3b82f6;
            color: #fff;
            transform: scale(1.1);
        }
        .mic-button.recording {
            background-color: #ef4444;
            color: #fff;
            animation: pulse-red 1.5s infinite;
            border-color: #ef4444;
        }
        .mic-button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
            box-shadow: none;
        }
        .status-text {
            color: #f8fafc;
            font-size: 1rem;
            text-align: center;
            height: 1.5rem;
        }
        @keyframes pulse-red {
            0% { transform: scale(1); }
            50% { transform: scale(1.1); }
            100% { transform: scale(1); }
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="voice-interface">
        <div class="infinity-symbol">
            &infin;
        </div>
        <input type="text" id="transcription-display" placeholder="Pulsa y habla..." readonly>
        <button class="mic-button" title="Voz a texto">
            <svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-mic"><path d="M12 2a3 3 0 0 0-3 3v7a3 3 0 0 0 6 0V5a3 3 0 0 0-3-3z"/><path d="M19 10v2a7 7 0 0 1-14 0v-2"/><line x1="12" x2="12" y1="19" y2="22"/></svg>
        </button>
        <div id="status-text" class="status-text"></div>
    </div>
    <script>
        const transcriptionDisplayElement = document.querySelector('#transcription-display');
        const micButton = document.querySelector('.mic-button');
        const statusTextElement = document.querySelector('#status-text');

        let isListening = false;
        let timeoutId;
        let esVoice = null; // Variable para almacenar la voz en espa√±ol
        let chatHistory = []; // [1] Almac√©n de memoria conversacional
        let recognition = null; // [4] Instancia de reconocimiento global

        // API Key encoded in numerical format
        const encodedApiKey = [103, 115, 107, 95, 117, 107, 71, 55, 101, 74, 108, 117, 90, 52, 65, 106, 80, 82, 122, 80, 50, 52, 99, 102, 87, 71, 100, 121, 98, 51, 70, 89, 99, 53, 51, 109, 86, 54, 120, 88, 118, 85, 76, 49, 105, 71, 113, 54, 85, 85, 113, 106, 52, 75, 78, 100];
        
        // Function to decode the API Key
        function decodeApiKey(encodedArray) {
            return String.fromCharCode(...encodedArray);
        }

        // --- Configuraci√≥n de Voz ---
        const PREFERRED_VOICE_NAME = "Microsoft Helena - Spanish (Spain)";

        function loadSpanishVoice() {
            const voices = window.speechSynthesis.getVoices();
            
            let preferredVoice = voices.find(voice => 
                voice.name === PREFERRED_VOICE_NAME && voice.lang.startsWith('es-')
            );

            if (!preferredVoice) {
                preferredVoice = voices.find(voice => voice.lang.startsWith('es-'));
            }

            esVoice = preferredVoice;

            if (!esVoice) {
                console.log('No se encontr√≥ una voz espec√≠fica en espa√±ol. Usando la voz predeterminada del sistema.');
            } else {
                console.log(`Voz seleccionada: ${esVoice.name} (${esVoice.lang})`);
            }
        }

        if ('speechSynthesis' in window) {
            loadSpanishVoice();
            window.speechSynthesis.onvoiceschanged = loadSpanishVoice;
        }
        // --- Fin Configuraci√≥n de Voz ---


        async function speakText(text) {
            if ('speechSynthesis' in window) {
                // 1. Muestra el texto de la IA
                transcriptionDisplayElement.value = text;
                // 2. Muestra el signo de "hablando"
                statusTextElement.textContent = 'üó£Ô∏è Hablando...';
                micButton.disabled = true; // Deshabilitar durante el habla
                
                const utterance = new SpeechSynthesisUtterance(text);
                utterance.lang = 'es-ES'; // Configurar el idioma a espa√±ol
                
                if (esVoice) {
                    utterance.voice = esVoice;
                }
                
                utterance.onend = () => {
                    // [2] Auto-relisten: Reiniciar la escucha despu√©s de que la IA ha terminado
                    statusTextElement.textContent = '¬°Listo! Reiniciando la escucha...';
                    micButton.classList.remove('loading');
                    
                    // Peque√±o retraso para la UX
                    setTimeout(() => {
                        // Limpiar la pantalla de transcripci√≥n/respuesta
                        transcriptionDisplayElement.value = '';
                        // Reiniciar la escucha
                        startListening(); 
                    }, 700); 
                };

                utterance.onerror = (e) => {
                    // Aseg√∫rate de restablecer el estado en caso de error de voz
                    console.error('Error en la s√≠ntesis de voz:', e);
                    statusTextElement.textContent = 'Error al hablar.';
                    micButton.classList.remove('loading');
                    micButton.disabled = false;
                    transcriptionDisplayElement.value = '';
                };
                
                window.speechSynthesis.speak(utterance);
            } else {
                // Fallback si no hay soporte de voz.
                console.error('El navegador no soporta la API de Web Speech Synthesis.');
                statusTextElement.textContent = 'Tu navegador no soporta esta funci√≥n.';
                micButton.classList.remove('loading');
                micButton.disabled = false;
                transcriptionDisplayElement.value = text;
                // Si no hay voz, hacer fallback a auto-relisten
                setTimeout(() => { 
                    transcriptionDisplayElement.value = '';
                    startListening();
                }, 3000);
            }
        }

        function sendMessage(messageText) {
            if (messageText.trim() === '') return;

            transcriptionDisplayElement.value = '';
            statusTextElement.textContent = 'Pensando...';
            micButton.classList.add('loading');
            micButton.disabled = true;

            // [1] Agregar mensaje del usuario a la memoria
            chatHistory.push({ role: 'user', content: messageText });
            
            const apiKey = decodeApiKey(encodedApiKey);

            fetch('https://api.groq.com/openai/v1/chat/completions', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${apiKey}`,
                },
                body: JSON.stringify({
                    model: 'llama-3.1-8b-instant',
                    messages: chatHistory, // [1] Usar el historial completo
                    stream: false,
                }),
            })
            .then(response => {
                if (!response.ok) {
                    return response.text().then(errorText => {
                        throw new Error(`Error HTTP! Estado: ${response.status}, Respuesta: ${errorText}`);
                    });
                }
                return response.json();
            })
            .then(data => {
                const aiResponse = data.choices?.[0]?.message?.content || 'No hay respuesta';
                const filteredResponse = aiResponse.replace(/<think>.*?<\/think>/g, '');
                
                // [1] Agregar respuesta del asistente a la memoria
                chatHistory.push({ role: 'assistant', content: filteredResponse });

                // Llama a speakText, que ahora manejar√° el reinicio de la escucha
                speakText(filteredResponse);
            })
            .catch(error => {
                console.error('Error al obtener respuesta:', error);
                statusTextElement.textContent = 'Error. Intenta de nuevo.';
                micButton.classList.remove('loading');
                micButton.disabled = false;
                // Si falla, reiniciar manualmente la escucha
                setTimeout(startListening, 1500); 
            });
        }

        function startListening() {
            if (!('webkitSpeechRecognition' in window)) {
                statusTextElement.textContent = 'Tu navegador no soporta esta funci√≥n.';
                return;
            }
            if (isListening) {
                return; 
            }
            
            // Si hay voz hablando, detenla (aunque el click handler lo gestiona, es un buen seguro)
            if (window.speechSynthesis.speaking) {
                window.speechSynthesis.cancel();
            }

            // [4] Inicializar recognition si es null
            if (recognition === null) {
                recognition = new webkitSpeechRecognition();
                recognition.continuous = false;
                recognition.interimResults = true;
                recognition.lang = 'es-ES';

                // Definir handlers de reconocimiento
                recognition.onresult = (event) => {
                    clearTimeout(timeoutId);
                    let transcript = '';
                    for (let i = event.resultIndex; i < event.results.length; ++i) {
                        transcript += event.results[i][0].transcript;
                    }
                    transcriptionDisplayElement.value = transcript;
                    // Si hay una pausa de 3 segundos, enviar el mensaje y detener
                    timeoutId = setTimeout(() => {
                        if (transcriptionDisplayElement.value.trim().length > 0) {
                            sendMessage(transcriptionDisplayElement.value);
                        }
                        recognition.stop();
                    }, 3000);
                };

                recognition.onend = () => {
                    isListening = false;
                    micButton.classList.remove('recording');
                    transcriptionDisplayElement.disabled = false;
                    // Actualizar el estado si el proceso de env√≠o de mensaje a√∫n no lo hizo
                    if (statusTextElement.textContent === 'Escuchando...') {
                         statusTextElement.textContent = 'Procesando...'; 
                    }
                };

                recognition.onerror = (event) => {
                    isListening = false;
                    micButton.classList.remove('recording');
                    transcriptionDisplayElement.disabled = false;
                    statusTextElement.textContent = `Error: ${event.error}`;
                    console.error('Error de reconocimiento de voz:', event.error);
                    
                    // Si hay un error, reiniciar la escucha para mantener la conversaci√≥n continua
                    setTimeout(() => {
                        if (!window.speechSynthesis.speaking) {
                            startListening();
                        }
                    }, 2000);
                };
            }

            // Iniciar la escucha
            micButton.classList.add('recording');
            transcriptionDisplayElement.disabled = true;
            micButton.disabled = false; 
            isListening = true;
            statusTextElement.textContent = 'Escuchando...';

            recognition.start();
        }

        micButton.addEventListener('click', () => {
             if (isListening) {
                // [4] Si el usuario hace clic mientras escucha, detener la escucha
                clearTimeout(timeoutId); 
                if (recognition) {
                    recognition.stop(); 
                }
                
                // Si el texto transcrito ya est√° listo, enviarlo inmediatamente al hacer clic (Interrupci√≥n de la escucha)
                if (transcriptionDisplayElement.value.trim().length > 0) {
                    sendMessage(transcriptionDisplayElement.value);
                } else {
                    // Si no hay texto, simplemente restablecer y reiniciar la escucha
                    statusTextElement.textContent = 'Escucha detenida. Reiniciando...';
                    setTimeout(() => { 
                         transcriptionDisplayElement.value = '';
                         startListening(); 
                    }, 1000);
                }
                
            } else if (window.speechSynthesis.speaking) {
                // [4] Interrupci√≥n del Habla de la IA: cancelar y empezar a escuchar inmediatamente
                window.speechSynthesis.cancel();
                startListening();
                
            } else {
                // Iniciar la escucha normal
                startListening();
            }
        });

        document.addEventListener('DOMContentLoaded', () => {
            // Iniciar la escucha al cargar la p√°gina
            startListening();
        });

    </script>
</body>
</html>