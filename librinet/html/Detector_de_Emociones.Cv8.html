```html
<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8">
  <title>Detector de Emociones</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.9.0/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection@1.0.2/dist/face-landmarks-detection.min.js"></script>
  <style>
    body {
      margin: 0;
      font-family: Arial, sans-serif;
      background: #111;
      color: #fff;
      display: flex;
      flex-direction: column;
      align-items: center;
    }
    h1 {
      margin: 20px 0 10px;
    }
    #video, #canvas {
      border-radius: 8px;
      transform: scaleX(-1);
    }
    #canvas {
      position: absolute;
      top: 0;
      left: 0;
      pointer-events: none;
    }
    .container {
      position: relative;
      width: 640px;
      height: 480px;
    }
    #emotion {
      margin-top: 15px;
      font-size: 1.4rem;
      letter-spacing: 1px;
    }
    button {
      margin-top: 15px;
      padding: 10px 20px;
      font-size: 1rem;
      border: none;
      border-radius: 4px;
      background: #0af;
      color: #fff;
      cursor: pointer;
    }
    button:hover {
      background: #08c;
    }
  </style>
</head>
<body>
  <h1>Detector de Emociones en Directo</h1>
  <div class="container">
    <video id="video" width="640" height="480" autoplay muted></video>
    <canvas id="canvas" width="640" height="480"></canvas>
  </div>
  <div id="emotion">Esperando emoción...</div>
  <button id="toggle">Detener</button>

  <script>
    const video = document.getElementById('video');
    const canvas = document.getElementById('canvas');
    const ctx = canvas.getContext('2d');
    const emotionText = document.getElementById('emotion');
    const toggleBtn = document.getElementById('toggle');

    let modelo;
    let animar = true;

    // Emociones según FaceMesh (aproximación simple)
    const EMOCIONES = ['Neutro', 'Feliz', 'Triste', 'Sorprendido', 'Enfadado'];

    async function iniciarCamara() {
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      video.srcObject = stream;
      return new Promise(res => video.onloadedmetadata = res);
    }

    async function cargarModelo() {
      modelo = await faceLandmarksDetection.load(
        faceLandmarksDetection.SupportedPackages.MediaPipeFaceMesh
      );
    }

    function detectarEmocion(predicciones) {
      if (!predicciones.length) return 'Neutro';
      // Simulación simple: elegimos una emoción aleatoria
      // En producción usarías un clasificador entrenado
      return EMOCIONES[Math.floor(Math.random() * EMOCIONES.length)];
    }

    async function detectar() {
      if (!animar) return;
      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
      const preds = await modelo.estimateFaces({ input: video });
      const emocion = detectarEmocion(preds);
      emotionText.textContent = `Emoción: ${emocion}`;
      requestAnimationFrame(detectar);
    }

    toggleBtn.addEventListener('click', () => {
      animar = !animar;
      toggleBtn.textContent = animar ? 'Detener' : 'Iniciar';
      if (animar) detectar();
    });

    (async () => {
      await cargarModelo();
      await iniciarCamara();
      detectar();
    })();
  </script>
</body>
</html>
```